<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Assistant</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <style>
        :root {
            --primary: #00f7ff;
            --secondary: #7b2cbf;
            --accent: #ff00e4;
            --bg-dark: #0f172a;
            --bg-darker: #0b1120;
            --text-light: #e2e8f0;
        }

        body {
            background-color: var(--bg-dark);
            color: var(--text-light);
            transition: all 0.3s ease;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
        }

        .glass-effect {
            background: rgba(15, 23, 42, 0.7);
            backdrop-filter: blur(10px);
            -webkit-backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.1);
        }

        .neon-glow {
            text-shadow: 0 0 8px var(--primary), 
                         0 0 16px var(--primary);
        }

        .pulse {
            animation: pulse 2s infinite;
        }

        @keyframes pulse {
            0% {
                box-shadow: 0 0 0 0 rgba(0, 247, 255, 0.7);
            }
            70% {
                box-shadow: 0 0 0 15px rgba(0, 247, 255, 0);
            }
            100% {
                box-shadow: 0 0 0 0 rgba(0, 247, 255, 0);
            }
        }

        .wave {
            position: relative;
            height: 80px;
            width: 80px;
            display: flex;
            justify-content: center;
            align-items: center;
        }

        .wave .dot {
            display: inline-block;
            width: 8px;
            height: 8px;
            border-radius: 50%;
            margin-right: 3px;
            background: var(--primary);
            animation: wave 1.3s linear infinite;
        }

        .wave .dot:nth-child(2) {
            animation-delay: -1.1s;
        }

        .wave .dot:nth-child(3) {
            animation-delay: -0.9s;
        }

        @keyframes wave {
            0%, 60%, 100% {
                transform: initial;
            }
            30% {
                transform: translateY(-15px);
            }
        }

        .voice-btn {
            transition: all 0.3s ease;
            box-shadow: 0 0 20px rgba(0, 247, 255, 0.3);
        }

        .voice-btn.active {
            background: var(--primary);
            color: var(--bg-darker);
            box-shadow: 0 0 30px var(--primary);
        }

        .response-text {
            border-left: 3px solid var(--primary);
            animation: textAppear 0.5s ease-out;
        }

        @keyframes textAppear {
            from {
                opacity: 0;
                transform: translateY(10px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .floating-orb {
            position: absolute;
            width: 200px;
            height: 200px;
            border-radius: 50%;
            background: radial-gradient(circle at 30% 30%, var(--primary), transparent 60%);
            filter: blur(30px);
            opacity: 0.3;
            z-index: -1;
            animation: float 15s infinite ease-in-out;
        }

        @keyframes float {
            0%, 100% {
                transform: translate(0, 0);
            }
            25% {
                transform: translate(50px, 50px);
            }
            50% {
                transform: translate(0, 100px);
            }
            75% {
                transform: translate(-50px, 50px);
            }
        }

        .typing-cursor {
            display: inline-block;
            width: 8px;
            height: 20px;
            background: var(--primary);
            animation: blink 1s infinite;
        }

        @keyframes blink {
            0%, 100% {
                opacity: 1;
            }
            50% {
                opacity: 0;
            }
        }

        #voiceSelect {
            background-color: rgba(15, 23, 42, 0.8);
            color: var(--text-light);
            border: 1px solid var(--primary);
        }
    </style>
</head>
<body class="overflow-hidden">
    <!-- Floating orbs for background effect -->
    <div class="floating-orb" style="top: 20%; left: 10%;"></div>
    <div class="floating-orb" style="top: 70%; left: 80%; animation-delay: 5s;"></div>
    <div class="floating-orb" style="top: 30%; left: 60%; animation-delay: 10s; width: 150px; height: 150px;"></div>

    <div class="min-h-screen flex flex-col items-center justify-center p-4">
        <div class="glass-effect rounded-2xl p-8 w-full max-w-3xl shadow-2xl">
            <!-- Header -->
            <div class="text-center mb-8">
                <h1 class="text-4xl font-bold mb-2 neon-glow" style="color: var(--primary)">A.R.I.A</h1>
                <p class="text-gray-400">Your futuristic voice-controlled companion, powered by smolLM2</p>
            </div>

            <!-- Status indicator -->
            <div class="flex justify-center mb-8">
                <div class="bg-gray-800 rounded-full px-4 py-2 flex items-center">
                    <div id="statusIndicator" class="w-3 h-3 rounded-full bg-gray-500 mr-2"></div>
                    <span id="statusText" class="text-sm">Ready</span>
                </div>
            </div>

            <!-- Voice selection dropdown -->
            <div class="flex justify-center mb-6">
                <div class="glass-effect rounded-xl p-3">
                    <label for="voiceSelect" class="text-sm text-gray-400 mb-1 block">Select Voice:</label>
                    <select id="voiceSelect" class="w-full py-2 px-3 rounded-lg">
                        <option value="">Loading voices...</option>
                    </select>
                </div>
            </div>

            <!-- Voice visualization -->
            <div class="flex justify-center mb-8">
                <div id="voiceVisualization" class="wave hidden">
                    <div class="dot"></div>
                    <div class="dot"></div>
                    <div class="dot"></div>
                </div>
            </div>

            <!-- Response area -->
            <div id="responseArea" class="glass-effect rounded-xl p-6 mb-8 min-h-32">
                <div class="flex items-start">
                    <div class="flex-shrink-0 h-10 w-10 rounded-full bg-cyan-900 flex items-center justify-center mr-3">
                        <i class="fas fa-robot text-cyan-300"></i>
                    </div>
                    <div class="flex-1">
                        <p class="font-semibold mb-2" style="color: var(--primary)">AI Response</p>
                        <div id="responseText" class="response-text pl-4">
                            <!-- Response will appear here -->
                        </div>
                    </div>
                </div>
            </div>

            <!-- Voice button -->
            <div class="flex justify-center">
                <button id="voiceButton" class="voice-btn w-24 h-24 rounded-full bg-gray-800 border-2 border-cyan-500 flex items-center justify-center text-3xl text-cyan-400 pulse">
                    <i class="fas fa-microphone"></i>
                </button>
            </div>

            <!-- Instructions -->
            <div class="mt-8 text-center text-gray-400 text-sm">
                <p>Press and hold the microphone button to speak</p>
                <p class="mt-1">Release to send your voice command</p>
            </div>

            <!-- Hidden form for Flask communication -->
            <form id="queryForm" action="/" method="POST" class="hidden">
                <input type="text" id="queryInput" name="query">
            </form>

            <!-- Fallback text input -->
            <div id="textInputFallback" class="mt-6">
                <div class="glass-effect rounded-xl p-4">
                    <input type="text" id="textCommand" placeholder="Type your command instead..." 
                        class="w-full bg-gray-800 border border-cyan-700 rounded px-4 py-2 text-white">
                    <button id="sendText" class="mt-2 bg-cyan-700 hover:bg-cyan-600 text-white py-2 px-4 rounded">
                        Send
                    </button>
                </div>
            </div>
        </div>

        <!-- Footer -->
        <div class="mt-8 text-center text-gray-500 text-sm">
            <p>Â© 2023 AI Voice Assistant | Futuristic Interface</p>
        </div>
    </div>

    <script>
        // DOM Elements
        const voiceButton = document.getElementById('voiceButton');
        const responseArea = document.getElementById('responseArea');
        const responseText = document.getElementById('responseText');
        const statusIndicator = document.getElementById('statusIndicator');
        const statusText = document.getElementById('statusText');
        const voiceVisualization = document.getElementById('voiceVisualization');
        const queryForm = document.getElementById('queryForm');
        const queryInput = document.getElementById('queryInput');
        const textInputFallback = document.getElementById('textInputFallback');
        const textCommand = document.getElementById('textCommand');
        const sendText = document.getElementById('sendText');
        const voiceSelect = document.getElementById('voiceSelect');

        // Check if response area should be hidden initially
        if (!responseText.textContent.trim()) {
            responseArea.classList.add('hidden');
        }

        // Speech recognition setup
        let recognition;
        let isListening = false;
        let finalTranscript = '';
        let speechSynthesis = window.speechSynthesis;
        let recognitionTimeout;
        let recognitionRetries = 0;
        const MAX_RETRIES = 3;
        let selectedVoice = null;
        
        const checkSpeechSupport = () => {
            return 'SpeechRecognition' in window || 'webkitSpeechRecognition' in window;
        };

        // Populate voice selection dropdown
        function loadVoices() {
            // Clear previous options
            voiceSelect.innerHTML = '';
            
            const voices = speechSynthesis.getVoices();
            
            if (voices.length === 0) {
                voiceSelect.innerHTML = '<option value="">No voices available</option>';
                return;
            }
            
            // Sort voices - English voices first, then others alphabetically
            const sortedVoices = [...voices].sort((a, b) => {
                const aIsEnglish = a.lang.startsWith('en');
                const bIsEnglish = b.lang.startsWith('en');
                
                if (aIsEnglish && !bIsEnglish) return -1;
                if (!aIsEnglish && bIsEnglish) return 1;
                
                return a.name.localeCompare(b.name);
            });
            
            // Add voices to dropdown
            sortedVoices.forEach(voice => {
                const option = document.createElement('option');
                option.value = voice.name;
                option.textContent = `${voice.name} (${voice.lang})`;
                voiceSelect.appendChild(option);
                
                // Select a default English voice
                if (!selectedVoice && (voice.name.includes('Google US English') || 
                    voice.name.includes('Samantha') || 
                    voice.name.includes('Google UK English Female'))) {
                    option.selected = true;
                    selectedVoice = voice;
                }
            });
            
            // If no default voice was selected, select the first one
            if (!selectedVoice && voices.length > 0) {
                voiceSelect.options[0].selected = true;
                selectedVoice = voices[0];
            }
            
            // Store user voice preference
            localStorage.setItem('selectedVoice', selectedVoice ? selectedVoice.name : '');
        }

        // Handle voice selection change
        voiceSelect.addEventListener('change', () => {
            const selectedName = voiceSelect.value;
            const voices = speechSynthesis.getVoices();
            selectedVoice = voices.find(voice => voice.name === selectedName) || null;
            
            // Store user preference
            if (selectedVoice) {
                localStorage.setItem('selectedVoice', selectedVoice.name);
            }
        });

        // Function to initialize speech recognition
        function initSpeechRecognition() {
            if (checkSpeechSupport()) {
                const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                recognition = new SpeechRecognition();
                recognition.continuous = false;
                recognition.interimResults = true;
                recognition.lang = 'en-US'; // Set language explicitly
                
                recognition.onstart = () => {
                    clearTimeout(recognitionTimeout); // Clear timeout when properly started
                    isListening = true;
                    voiceButton.classList.add('active');
                    statusIndicator.classList.remove('bg-gray-500', 'bg-red-500');
                    statusIndicator.classList.add('bg-green-500');
                    statusText.textContent = 'Listening...';
                    voiceVisualization.classList.remove('hidden');
                    finalTranscript = '';
                    recognitionRetries = 0; // Reset retries counter
                };
                
                recognition.onresult = (event) => {
                    let interimTranscript = '';
                    
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        const transcript = event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            finalTranscript += transcript;
                        } else {
                            interimTranscript += transcript;
                        }
                    }
                    
                    // Show interim results
                    if (interimTranscript || finalTranscript) {
                        responseArea.classList.remove('hidden');
                        responseText.innerHTML = `<span class="text-gray-400">You said: "${interimTranscript || finalTranscript}"</span>${interimTranscript ? '<span class="typing-cursor"></span>' : ''}`;
                    }
                };
                
                recognition.onerror = (event) => {
                    console.error('Speech recognition error', event.error);
                    isListening = false;
                    voiceButton.classList.remove('active');
                    voiceVisualization.classList.add('hidden');
                    statusIndicator.classList.remove('bg-green-500');
                    statusIndicator.classList.add('bg-red-500');
                    
                    // Special handling for permission errors
                    if (event.error === 'not-allowed') {
                        statusText.textContent = 'Microphone permission denied';
                    } else if (event.error === 'network') {
                        statusText.textContent = 'Network error. Check your connection.';
                    } else {
                        statusText.textContent = 'Error: ' + event.error;
                    }
                    
                    setTimeout(resetStatus, 3000);
                };
                
                recognition.onend = () => {
                    clearTimeout(recognitionTimeout);
                    isListening = false;
                    voiceButton.classList.remove('active');
                    voiceVisualization.classList.add('hidden');
                    
                    if (finalTranscript) {
                        processVoiceCommand(finalTranscript);
                    } else if (recognitionRetries < MAX_RETRIES) {
                        // Recognition ended without results, try again
                        recognitionRetries++;
                        statusText.textContent = `No speech detected, retrying (${recognitionRetries}/${MAX_RETRIES})...`;
                        setTimeout(() => {
                            try {
                                recognition.start();
                            } catch (err) {
                                console.error('Failed to restart recognition:', err);
                                resetStatus();
                            }
                        }, 1000);
                    } else {
                        statusText.textContent = 'No speech detected. Please try again.';
                        setTimeout(resetStatus, 2000);
                    }
                };
                
                // Button event handlers
                voiceButton.addEventListener('mousedown', startListening);
                voiceButton.addEventListener('touchstart', startListening);
                voiceButton.addEventListener('mouseup', stopListening);
                voiceButton.addEventListener('touchend', stopListening);
                voiceButton.addEventListener('mouseleave', stopListening);
                
                return true;
            } else {
                console.error('Speech recognition not supported in this browser');
                voiceButton.disabled = true;
                voiceButton.innerHTML = '<i class="fas fa-microphone-slash"></i>';
                statusIndicator.classList.remove('bg-gray-500');
                statusIndicator.classList.add('bg-red-500');
                statusText.textContent = 'Voice not supported';
                return false;
            }
        }

        // Initialize speech recognition and voices when page loads
        let speechInitialized = false;
        window.addEventListener('DOMContentLoaded', () => {
            // Load available voices
            if ('speechSynthesis' in window) {
                // Chrome loads voices asynchronously
                speechSynthesis.onvoiceschanged = () => {
                    loadVoices();
                    
                    // Try to restore user's preferred voice
                    const savedVoice = localStorage.getItem('selectedVoice');
                    if (savedVoice) {
                        const voices = speechSynthesis.getVoices();
                        const matchedVoice = voices.find(v => v.name === savedVoice);
                        if (matchedVoice) {
                            selectedVoice = matchedVoice;
                            Array.from(voiceSelect.options).some(option => {
                                if (option.value === savedVoice) {
                                    option.selected = true;
                                    return true;
                                }
                                return false;
                            });
                        }
                    }
                };
                
                // Initial load for Firefox which doesn't always trigger onvoiceschanged
                const voices = speechSynthesis.getVoices();
                if (voices.length > 0) {
                    loadVoices();
                }
            } else {
                voiceSelect.innerHTML = '<option value="">Speech synthesis not supported</option>';
                voiceSelect.disabled = true;
            }
            
            speechInitialized = initSpeechRecognition();
            
            // Handle any existing response from Flask
            const flaskResponse = {{ response|tojson|safe }} || "";
            if (flaskResponse && flaskResponse.trim().length > 0) {
                responseArea.classList.remove('hidden');
                responseText.textContent = flaskResponse;
                speakResponse(flaskResponse);
            }

            // Try to test speech recognition without actually listening
            if (speechInitialized) {
                try {
                    // Just ping the recognition system to trigger permission requests
                    const testRecognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                    testRecognition.continuous = false;
                    testRecognition.interimResults = false;
                    testRecognition.maxAlternatives = 1;
                    
                    let testTimeout = setTimeout(() => {
                        try { testRecognition.stop(); } catch(e) {}
                    }, 1000);
                    
                    testRecognition.onstart = () => {
                        clearTimeout(testTimeout);
                        setTimeout(() => {
                            try { testRecognition.stop(); } catch(e) {}
                        }, 100);
                    };
                    
                    testRecognition.start();
                } catch(e) {
                    console.warn('Speech recognition test failed:', e);
                }
            }
        });
        
        function startListening(e) {
            e.preventDefault();
            if (!isListening && recognition) {
                try {
                    recognition.start();
                    // Set timeout in case recognition doesn't trigger onstart
                    recognitionTimeout = setTimeout(() => {
                        if (!isListening) {
                            console.warn("Recognition didn't start properly, retrying...");
                            try {
                                recognition.stop();
                                setTimeout(() => {
                                    try {
                                        recognition.start();
                                    } catch(err) {
                                        console.error('Failed to restart recognition:', err);
                                        resetStatus();
                                    }
                                }, 300);
                            } catch (err) {
                                console.error('Failed to stop non-started recognition:', err);
                                resetStatus();
                            }
                        }
                    }, 2000);
                } catch (err) {
                    console.error('Recognition error:', err);
                    statusIndicator.classList.remove('bg-gray-500');
                    statusIndicator.classList.add('bg-red-500');
                    statusText.textContent = 'Error starting recognition';
                    setTimeout(resetStatus, 3000);
                }
            }
        }
        
        function stopListening(e) {
            e.preventDefault();
            if (recognition) {
                try {
                    recognition.stop();
                } catch (err) {
                    console.error('Error stopping recognition:', err);
                }
            }
        }
        
        function resetStatus() {
            statusIndicator.classList.remove('bg-green-500', 'bg-red-500', 'bg-yellow-500', 'bg-blue-500');
            statusIndicator.classList.add('bg-gray-500');
            statusText.textContent = 'Ready';
        }
        
        function processVoiceCommand(command) {
            // Show recognized text
            responseArea.classList.remove('hidden');
            responseText.innerHTML = `<span class="text-gray-400">You said: "${command}"</span>`;
            
            // Set query in form and submit to Flask
            queryInput.value = command;
            
            // Simulate AI thinking
            statusIndicator.classList.remove('bg-green-500');
            statusIndicator.classList.add('bg-yellow-500');
            statusText.textContent = 'Processing...';
            
            // Submit form to Flask backend
            queryForm.submit();
        }

        // Text input fallback handlers
        sendText.addEventListener('click', () => {
            const command = textCommand.value;
            if (command.trim()) {
                processVoiceCommand(command);
                textCommand.value = '';
            }
        });

        textCommand.addEventListener('keypress', (e) => {
            if (e.key === 'Enter') {
                const command = textCommand.value;
                if (command.trim()) {
                    processVoiceCommand(command);
                    textCommand.value = '';
                }
            }
        });

        function speakResponse(text) {
            if (speechSynthesis) {
                // Cancel any ongoing speech
                speechSynthesis.cancel();
                
                const utterance = new SpeechSynthesisUtterance(text);
                
                // Use the selected voice if available
                if (selectedVoice) {
                    utterance.voice = selectedVoice;
                } else {
                    // Get available voices
                    let voices = speechSynthesis.getVoices();
                    
                    // If voices array is empty, wait for the onvoiceschanged event
                    if (voices.length === 0) {
                        speechSynthesis.onvoiceschanged = () => {
                            voices = speechSynthesis.getVoices();
                            setDefaultVoice();
                        };
                    } else {
                        setDefaultVoice();
                    }
                    
                    function setDefaultVoice() {
                        // Find a good English voice
                        const preferredVoice = voices.find(voice => 
                            voice.name.includes('Google US English') || 
                            voice.name.includes('Samantha') || 
                            voice.name.includes('Google UK English Female') || 
                            voice.name.includes('en-US')
                        ) || voices.find(voice => voice.lang.includes('en')) || voices[0];
                        
                        if (preferredVoice) {
                            utterance.voice = preferredVoice;
                            selectedVoice = preferredVoice;
                            
                            // Update dropdown to match
                            for (let i = 0; i < voiceSelect.options.length; i++) {
                                if (voiceSelect.options[i].value === preferredVoice.name) {
                                    voiceSelect.options[i].selected = true;
                                    break;
                                }
                            }
                        }
                    }
                }
                
                utterance.rate = 1.1;
                utterance.pitch = 1.1;
                
                // Start speaking
                speechSynthesis.speak(utterance);
                
                // Visual feedback
                statusIndicator.classList.remove('bg-yellow-500');
                statusIndicator.classList.add('bg-blue-500');
                statusText.textContent = 'Speaking';
                
                utterance.onend = () => {
                    resetStatus();
                };
            }
        }
    </script>
</body>
</html>